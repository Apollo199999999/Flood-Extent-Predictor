{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa221250",
   "metadata": {},
   "source": [
    "# Generating tiles from raw TIFF files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825a625",
   "metadata": {},
   "source": [
    "This notebook downloads the TIFF files obtained from Google Earth Engine, preprocesses the images into 500x500 (for features) or 50x50 (for labels) tiles for training and testing, and saves everything into Numpy files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2882c3d",
   "metadata": {},
   "source": [
    "## 0. Install libraries and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee1c2f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install huggingface_hub rasterio affine tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaf1a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio import windows\n",
    "from rasterio.warp import reproject\n",
    "from rasterio.enums import Resampling\n",
    "from affine import Affine\n",
    "from tqdm import tqdm\n",
    "from numpy.lib.format import open_memmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ba49d9",
   "metadata": {},
   "source": [
    "## 1. Configure settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace8a1d0",
   "metadata": {},
   "source": [
    "### Declare constants for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd18cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directory to download TIFF files to and processed tiles. \n",
    "# This will require a few 100GB of space so uh make sure you do this on a 1 TB external drive or something\n",
    "WORK_DIR  = Path(\"E:\\\\RSI09\\\\v4\")\n",
    "# Make some directories to process files\n",
    "DATASET_DIR   = WORK_DIR / \"tiff\"\n",
    "TILES_DIR = WORK_DIR / \"tiles\"\n",
    "\n",
    "for p in [DATASET_DIR, TILES_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "# To prevent significant class imbalance, we only train on tiles that have minimum number of pixels identified as flood.\n",
    "# This threshold is the same used in MMFlood\n",
    "MIN_FLOOD_PCT = 2.0   \n",
    "\n",
    "# Size of tiles\n",
    "PRED_TILE = (500, 500)  # predictor tile size (50 m * 10)\n",
    "LBL_TILE  = (50, 50)    # label tile size (500 m / 10)\n",
    "\n",
    "# Filenames to identify which raw tiffs belong to the test dataset\n",
    "TEST_FILENAMES = [\"99.1717616_17.1286267\", \"4.7049263_9.9129092\"]\n",
    "\n",
    "# List of features in the dataset\n",
    "# Make sure \"label\" is the last item\n",
    "FEATURES = [\"dem\", \"slope\", \"ndvi\", \"lc\", \"water\", \"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604e412c",
   "metadata": {},
   "source": [
    "## 2. Download and group files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dea1c94",
   "metadata": {},
   "source": [
    "### Download data if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bff6e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from HuggingFace\n",
    "# snapshot_download(repo_id=\"MatthiasWang/geotiffs\", repo_type=\"dataset\", local_dir=DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e54bd82",
   "metadata": {},
   "source": [
    "### Group files belonging to a single flood event together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1824f2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 locations with complete rasters (grouped).\n"
     ]
    }
   ],
   "source": [
    "# Google Earth Engine will split a single export into multiple files if the export is too large\n",
    "# We need to keep track of all these split files\n",
    "# The filenames are in the following format:\n",
    "# {folder}_flood_<lon>_<lat>-0000-0000...\n",
    "\n",
    "# RegEx for the filename\n",
    "BASE_KEY_RE = re.compile(\n",
    "    r\"^[a-z]+_flood_(-?\\d+(?:\\.\\d+)?)_(-?\\d+(?:\\.\\d+)?)(?:-\\d+)*$\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Extract base key \"<lon>_<lat>\" from \"{folder}_flood_<lon>_<lat>-0000-0000...\" (stem)\n",
    "def extract_base_key(stem):\n",
    "    m = BASE_KEY_RE.match(stem)\n",
    "    \n",
    "    if m: \n",
    "        # Filename is in the format \"{folder}_flood_<lon>_<lat>-0000-0000...\"\n",
    "        return f\"{m.group(1)}_{m.group(2)}\"\n",
    "    \n",
    "    # Filename is in the format \"{folder}_flood_<lon>_<lat>\"\n",
    "    m2 = re.match(r\"^[a-z]+_flood_(.+)$\", stem, flags=re.IGNORECASE)\n",
    "    return m2.group(1) if m2 else None\n",
    "\n",
    "# Next, we need a dictionary to keep track of all the floods, for each flood event, for each feature\n",
    "# i.e., we need a dictionary like so:\n",
    "'''\n",
    "maps: {\n",
    "    \"dem\": {\n",
    "        -98.7136208_49.7307341: [dem_flood_-98.7136208_49.7307341-0000-0000, dem_flood_-98.7136208_49.7307341-0000-0032, ...],\n",
    "        -93.8267856_30.2350467: [dem_flood_-93.8267856_30.2350467-0000-0000, dem_flood_-93.8267856_30.2350467-0000-0032, ...],\n",
    "        ...\n",
    "    }\n",
    "\n",
    "    \"slope\": {\n",
    "        -98.7136208_49.7307341: [slope_flood_-98.7136208_49.7307341-0000-0000, slope_flood_-98.7136208_49.7307341-0000-0032, ...],\n",
    "        -93.8267856_30.2350467: [slope_flood_-93.8267856_30.2350467-0000-0000, slope_flood_-93.8267856_30.2350467-0000-0032, ...],\n",
    "        ...\n",
    "    }\n",
    "\n",
    "    ...\n",
    "} \n",
    "'''\n",
    "\n",
    "# Valid file extensions\n",
    "VALID_EXT = {\".tif\", \".tiff\"}\n",
    "\n",
    "def list_folder_files_grouped(folder):\n",
    "    out = {}\n",
    "\n",
    "    # This shouldn't happen, but just in case\n",
    "    if not folder.exists(): return out\n",
    "\n",
    "    for p in folder.iterdir():\n",
    "        if not p.is_file(): continue\n",
    "\n",
    "        if p.suffix.lower() not in VALID_EXT and \".\" in p.name:\n",
    "            pass\n",
    "\n",
    "        base = extract_base_key(p.stem)\n",
    "\n",
    "        # If extract_base_key returns nothing\n",
    "        if not base: continue\n",
    "\n",
    "        out.setdefault(base, []).append(p)\n",
    "    return out\n",
    "\n",
    "folders = {k: DATASET_DIR / k for k in FEATURES}\n",
    "maps = {k: list_folder_files_grouped(v) for k, v in folders.items()}\n",
    "\n",
    "# Keep only locations that exist in ALL folders\n",
    "# Keys is an array storing \"<lon>_<lat>\" for all valid locations\n",
    "keys = set.intersection(*(set(m.keys()) for m in maps.values())) if all(maps.values()) else set()\n",
    "print(f\"Found {len(keys)} locations with complete rasters (grouped).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2f0009",
   "metadata": {},
   "source": [
    "## 3. Extract tiles from maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07f8113",
   "metadata": {},
   "source": [
    "### Helper functions to reproject onto fixed tile sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11970c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject_many_into_tile(paths, dst_transform, dst_crs, dst_shape, *, categorical):\n",
    "    \"\"\"\n",
    "    Reproject multiple source rasters into a single destination tile (dst_shape).\n",
    "    Later rasters do NOT erase existing valid values with NaN:\n",
    "      - We reproject each source into a temp tile, then merge where temp is finite and dst is NaN.\n",
    "    This keeps memory use small and avoids OOM.\n",
    "    \"\"\"\n",
    "    H, W = dst_shape\n",
    "    dst = np.full((H, W), np.nan, dtype=np.float32)\n",
    "    for p in paths:\n",
    "        try:\n",
    "            with rasterio.open(p) as src:\n",
    "                temp = np.full((H, W), np.nan, dtype=np.float32)\n",
    "                reproject(\n",
    "                    source=rasterio.band(src, 1),\n",
    "                    destination=temp,\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=dst_transform,\n",
    "                    dst_crs=dst_crs,\n",
    "                    resampling=(Resampling.nearest if categorical else Resampling.bilinear),\n",
    "                    dst_nodata=np.nan,\n",
    "                    num_threads=2\n",
    "                )\n",
    "                # Merge: fill only where dst is NaN and temp is finite\n",
    "                m = ~np.isnan(temp) & np.isnan(dst)\n",
    "                if m.any():\n",
    "                    dst[m] = temp[m]\n",
    "        except Exception:\n",
    "            continue\n",
    "    return dst\n",
    "\n",
    "def binarise(arr):\n",
    "    return (arr > 0).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5836e5fd",
   "metadata": {},
   "source": [
    "### Function to extract tiles for training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "936efe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all_streaming(keys, maps, tiles_dir):\n",
    "    tiles_dir.mkdir(parents=True, exist_ok=True)\n",
    "    kept_total, seen_total = 0, 0\n",
    "    thr = MIN_FLOOD_PCT / 100.0\n",
    "\n",
    "    for key in tqdm(sorted(keys), desc=\"Preprocessing (streaming tiles)\"):\n",
    "        dem_list   = maps[\"dem\"][key]\n",
    "        slope_list = maps[\"slope\"][key]\n",
    "        ndvi_list  = maps[\"ndvi\"][key]\n",
    "        lc_list    = maps[\"lc\"][key]\n",
    "        water_list = maps[\"water\"][key]\n",
    "        lbl_list   = maps[\"label\"][key]\n",
    "\n",
    "        # Process each DEM chunk individually\n",
    "        for dem_path in dem_list:\n",
    "            with rasterio.open(dem_path) as dem_src:\n",
    "                crs = dem_src.crs\n",
    "                transform = dem_src.transform\n",
    "                height, width = dem_src.height, dem_src.width\n",
    "\n",
    "                # Only full 500x500 tiles\n",
    "                Hc = (height // PRED_TILE[0]) * PRED_TILE[0]\n",
    "                Wc = (width  // PRED_TILE[1]) * PRED_TILE[1]\n",
    "                if Hc == 0 or Wc == 0:\n",
    "                    continue\n",
    "\n",
    "                for r0 in range(0, Hc, PRED_TILE[0]):\n",
    "                    for c0 in range(0, Wc, PRED_TILE[1]):\n",
    "                        seen_total += 1\n",
    "\n",
    "                        # Use a window to read 500x500 chunks\n",
    "                        win = windows.Window(c0, r0, *PRED_TILE)\n",
    "                        tile_transform = windows.transform(win, transform)\n",
    "\n",
    "                        # DEM\n",
    "                        dem = dem_src.read(1, window=win, masked=True).filled(np.nan).astype(np.float32)\n",
    "\n",
    "                        # Label on coarse grid (50×50)\n",
    "                        coarse_transform = tile_transform * Affine.scale(10, 10)\n",
    "                        y_coarse = reproject_many_into_tile(lbl_list, coarse_transform, crs, LBL_TILE, categorical=True)\n",
    "                        y = binarise(y_coarse)\n",
    "\n",
    "                        # We only enforce minimum flood threshold for training dataset\n",
    "                        data_split = \"test\"\n",
    "                        flood_ratio = float(np.nanmean(y))  # y has 0/1; NaNs rare but guard anyway\n",
    "                        if key not in TEST_FILENAMES:\n",
    "                            data_split = \"train\"\n",
    "                            if not np.isfinite(flood_ratio) or flood_ratio < thr:\n",
    "                                continue  # reject  \n",
    "\n",
    "                        # Slope / NDVI / LC / Water\n",
    "                        slope = reproject_many_into_tile(slope_list, tile_transform, crs, PRED_TILE, categorical=False)\n",
    "                        ndvi = reproject_many_into_tile(ndvi_list, tile_transform, crs, PRED_TILE, categorical=False)\n",
    "                        lc = reproject_many_into_tile(lc_list, tile_transform, crs, PRED_TILE, categorical=True)\n",
    "                        water = reproject_many_into_tile(water_list, tile_transform, crs, PRED_TILE, categorical=False)\n",
    "\n",
    "                        # For NDVI, Slope, Water, divide by 1000 to convert back to float32\n",
    "                        slope /= 1000\n",
    "                        ndvi /= 1000\n",
    "                        water /= 1000\n",
    "\n",
    "                        # Make sure the stacked features are in the same order as FEATURES array\n",
    "                        x = np.stack([dem, slope, ndvi, lc, water], axis=0).astype(np.float32)\n",
    "                        x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "                        meta = {\n",
    "                            \"key\": key,\n",
    "                            \"dem_chunk\": Path(dem_path).name,\n",
    "                            \"r\": int(r0), \"c\": int(c0),\n",
    "                            \"Hc\": int(Hc), \"Wc\": int(Wc),\n",
    "                            \"predictor_tile_shape\": [len(FEATURES) - 1, PRED_TILE[0], PRED_TILE[1]],\n",
    "                            \"label_tile_shape\": [LBL_TILE[0], LBL_TILE[1]],\n",
    "                            \"flood_ratio\": flood_ratio,\n",
    "                        }\n",
    "\n",
    "                        tile_name = f\"{key}__{Path(dem_path).stem}__r{r0}c{c0}_{data_split}\"\n",
    "                        np.savez_compressed(tiles_dir / f\"{tile_name}.npz\",\n",
    "                                            x=x.astype(np.float32),\n",
    "                                            y=y.astype(np.uint8),\n",
    "                                            meta=json.dumps(meta))\n",
    "                        kept_total += 1\n",
    "\n",
    "    print(f\"Saved {kept_total} / {seen_total} tiles \"\n",
    "          f\"(kept {100.0 * kept_total / seen_total:.1f}%)\"\n",
    "          f\"to {tiles_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "835fefe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing (streaming tiles): 100%|██████████| 8/8 [4:01:36<00:00, 1812.01s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 7811 / 115759 tiles (kept 6.7%)to E:\\RSI09\\v4\\tiles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocess_all_streaming(list(keys), maps, TILES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5b7637",
   "metadata": {},
   "source": [
    "## 4. Save training and testing dataset to Numpy files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2f2a95",
   "metadata": {},
   "source": [
    "### Load the metadata for each tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb19e804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decode_meta(meta_raw):\n",
    "    # meta was saved as JSON string; handle various npz dtypes robustly\n",
    "    try:\n",
    "        if hasattr(meta_raw, \"dtype\") and meta_raw.dtype.kind in \"OUS\":\n",
    "            return json.loads(meta_raw.item())\n",
    "        try:\n",
    "            return json.loads(meta_raw.tobytes().decode(\"utf-8\"))\n",
    "        except Exception:\n",
    "            return json.loads(str(meta_raw))\n",
    "    except Exception:\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a0f9fb",
   "metadata": {},
   "source": [
    "### Function to stack tiles into a single Numpy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad461380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tile_paths is a list of .npz files\n",
    "# out_base is the stem of the filepath to save the stacked npz files to e.g /kaggle/working/flood_stack\n",
    "def save_numpy_stack_from_tiles(tile_paths, out_base):\n",
    "    # Define paths\n",
    "    N = len(tile_paths)\n",
    "    X_path = f\"{out_base}_X.npy\"\n",
    "    Y_path = f\"{out_base}_Y.npy\"\n",
    "    meta_path = f\"{out_base}_meta.jsonl\"\n",
    "    stats_path = f\"{out_base}_stats.json\"\n",
    "\n",
    "    # Create the output npy files\n",
    "    X_mm = open_memmap(X_path, mode=\"w+\", dtype=np.float32, shape=(N, len(FEATURES) - 1, PRED_TILE[0], PRED_TILE[1]))\n",
    "    Y_mm = open_memmap(Y_path, mode=\"w+\", dtype=np.uint8,   shape=(N, LBL_TILE[0], LBL_TILE[1]))\n",
    "\n",
    "    # Used to compute normalization/standardization stats (mean, variance, etc.)\n",
    "    sums = np.zeros(len(FEATURES) - 1, dtype=np.float64)\n",
    "    sumsq = np.zeros(len(FEATURES) - 1, dtype=np.float64)\n",
    "    counts = np.zeros(len(FEATURES) - 1, dtype=np.int64)\n",
    "\n",
    "    with open(meta_path, \"w\") as fmeta:\n",
    "        for i, p in enumerate(tqdm(tile_paths, desc=\"Collating tiles to NumPy\")):\n",
    "            rec = np.load(p, allow_pickle=False)\n",
    "            # Feature maps\n",
    "            x = rec[\"x\"]\n",
    "            # Label\n",
    "            y = rec[\"y\"]\n",
    "            m = _decode_meta(rec[\"meta\"])\n",
    "            m[\"predictor_tile_shape\"] = [len(FEATURES) - 1, PRED_TILE[0], PRED_TILE[1]]\n",
    "\n",
    "            X_mm[i] = x.astype(np.float32)\n",
    "            Y_mm[i] = y.astype(np.uint8)\n",
    "\n",
    "            for c in range(len(FEATURES) - 1):\n",
    "                feature = x[c]\n",
    "                finite = np.isfinite(feature)\n",
    "\n",
    "                # No NaN values\n",
    "                if finite.any():\n",
    "                    vals = feature[finite].astype(np.float64)\n",
    "                    sums[c]  += vals.sum()\n",
    "                    sumsq[c] += (vals * vals).sum()\n",
    "                    counts[c] += vals.size\n",
    "\n",
    "            fmeta.write(json.dumps(m) + \"\\n\")\n",
    "\n",
    "    del X_mm, Y_mm  # flush\n",
    "\n",
    "    means = (sums / np.maximum(counts, 1)).tolist()\n",
    "    variances = (sumsq / np.maximum(counts, 1) - np.square(sums / np.maximum(counts, 1)))\n",
    "    stds = np.sqrt(np.maximum(variances, 0)).tolist()\n",
    "\n",
    "    with open(stats_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"N\": N,\n",
    "            \"channels\": FEATURES[:-1],\n",
    "            \"mean\": means,\n",
    "            \"std\": stds,\n",
    "            \"counts\": counts.tolist()\n",
    "        }, f, indent=2)\n",
    "\n",
    "    return {\"X\": X_path, \"Y\": Y_path, \"meta\": meta_path, \"stats\": stats_path, \"N\": N}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c7a60d",
   "metadata": {},
   "source": [
    "### Save train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f743b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All paths where tiles are stored\n",
    "full_tile_paths = sorted(Path(TILES_DIR).glob(\"*.npz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9527e322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collating tiles to NumPy: 100%|██████████| 6603/6603 [22:21<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': 'E:\\\\RSI09\\\\v4\\\\train_X.npy', 'Y': 'E:\\\\RSI09\\\\v4\\\\train_Y.npy', 'meta': 'E:\\\\RSI09\\\\v4\\\\train_meta.jsonl', 'stats': 'E:\\\\RSI09\\\\v4\\\\train_stats.json', 'N': 6603}\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving training data...\")\n",
    "train_paths = []\n",
    "for path in full_tile_paths:\n",
    "    if \"train\" in str(path):\n",
    "        train_paths.append(str(path))\n",
    "\n",
    "STACK_BASE = WORK_DIR / \"train\"\n",
    "stack_info = save_numpy_stack_from_tiles(train_paths, STACK_BASE)\n",
    "print(stack_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bcafc4",
   "metadata": {},
   "source": [
    "### Save test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68b972f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving testing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collating tiles to NumPy: 100%|██████████| 608/608 [01:14<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': 'E:\\\\RSI09\\\\v4\\\\test_99.1717616_17.1286267_X.npy', 'Y': 'E:\\\\RSI09\\\\v4\\\\test_99.1717616_17.1286267_Y.npy', 'meta': 'E:\\\\RSI09\\\\v4\\\\test_99.1717616_17.1286267_meta.jsonl', 'stats': 'E:\\\\RSI09\\\\v4\\\\test_99.1717616_17.1286267_stats.json', 'N': 608}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collating tiles to NumPy: 100%|██████████| 600/600 [01:10<00:00,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': 'E:\\\\RSI09\\\\v4\\\\test_4.7049263_9.9129092_X.npy', 'Y': 'E:\\\\RSI09\\\\v4\\\\test_4.7049263_9.9129092_Y.npy', 'meta': 'E:\\\\RSI09\\\\v4\\\\test_4.7049263_9.9129092_meta.jsonl', 'stats': 'E:\\\\RSI09\\\\v4\\\\test_4.7049263_9.9129092_stats.json', 'N': 600}\n"
     ]
    }
   ],
   "source": [
    "# For testing dataset, we want each testing example to be exported as its own npz file\n",
    "print(\"Saving testing data...\")\n",
    "for test_filename in TEST_FILENAMES:\n",
    "    test_paths = []\n",
    "    \n",
    "    for path in full_tile_paths:\n",
    "        if test_filename in str(path) and \"test\" in str(path):\n",
    "            test_paths.append(str(path))\n",
    "\n",
    "    STACK_BASE = WORK_DIR / f\"test_{test_filename}\"\n",
    "    stack_info = save_numpy_stack_from_tiles(test_paths, STACK_BASE)\n",
    "    print(stack_info)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
